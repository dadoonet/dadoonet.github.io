<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>tutorial on David Pilato</title><link>https://david.pilato.fr/categories/tutorial/</link><description>Recent content in tutorial on David Pilato</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>&amp;copy; 2022, David Pilato, France</copyright><lastBuildDate>Thu, 10 Jan 2019 00:00:00 +0000</lastBuildDate><atom:link href="https://david.pilato.fr/categories/tutorial/index.xml" rel="self" type="application/rss+xml"/><item><title>From a startup to a listed company. 6 years of fun!</title><link>https://david.pilato.fr/blog/2019-01-10-from-a-startup-to-a-listed-company-6-years-of-fun/</link><pubDate>Thu, 10 Jan 2019 00:00:00 +0000</pubDate><guid>https://david.pilato.fr/blog/2019-01-10-from-a-startup-to-a-listed-company-6-years-of-fun/</guid><description>When I joined Elastic (formerly Elasticsearch) it was a startup with 10 employees + the founders. As one of those first employees I was invited (with #elkie and my wife) to the NYSE event where Elastic went listed as ESTC symbol.
Some of us there (Rashid, Karel, Myself, Igor, Costin, Luca, Clinton). Yeah. You are not probably used to see us wearing a suit! :)
If you want to read again my story, it&amp;rsquo;s there:</description></item><item><title>Enriching your postal addresses with Elastic stack - part 3</title><link>https://david.pilato.fr/blog/2018-03-24-enriching-your-postal-addresses-with-elastic-stack-part-3/</link><pubDate>Sat, 24 Mar 2018 00:00:00 +0000</pubDate><guid>https://david.pilato.fr/blog/2018-03-24-enriching-your-postal-addresses-with-elastic-stack-part-3/</guid><description>This blog post is part of a series of 3:
Importing Bano dataset with Logstash Using Logstash to lookup for addresses in Bano index Using Logstash to enrich an existing dataset with Bano In the previous post, we described how we can transform a postal address to a normalized one with also the geo location point or transform a geo location point to a postal address.
Let&amp;rsquo;s say we have an existing dataset we want to enrich.</description></item><item><title>Enriching your postal addresses with Elastic stack - part 2</title><link>https://david.pilato.fr/blog/2018-03-23-enriching-your-postal-addresses-with-elastic-stack-part-2/</link><pubDate>Fri, 23 Mar 2018 10:20:28 +0200</pubDate><guid>https://david.pilato.fr/blog/2018-03-23-enriching-your-postal-addresses-with-elastic-stack-part-2/</guid><description>This blog post is part of a series of 3:
Importing Bano dataset with Logstash Using Logstash to lookup for addresses in Bano index Using Logstash to enrich an existing dataset with Bano In the previous post, we described how we indexed data coming from the BANO project so we now have indices containing all the french postal addresses.
Let&amp;rsquo;s see what we can do now with this dataset.
Searching for addresses Good. Can we use a search engine to search?</description></item><item><title>Enriching your postal addresses with Elastic stack - part 1</title><link>https://david.pilato.fr/blog/2018-03-22-enriching-your-postal-addresses-with-elastic-stack-part-1/</link><pubDate>Thu, 22 Mar 2018 15:31:49 +0100</pubDate><guid>https://david.pilato.fr/blog/2018-03-22-enriching-your-postal-addresses-with-elastic-stack-part-1/</guid><description>This blog post is part of a series of 3:
Importing Bano dataset with Logstash Using Logstash to lookup for addresses in Bano index Using Logstash to enrich an existing dataset with Bano I&amp;rsquo;m not really sure why, but I love the postal address use case. Often in my career I had to deal with that information. Very often the information is not well formatted so it&amp;rsquo;s hard to find the information you need when you have as an input a not so nice dataset.</description></item><item><title>5 years. What a milestone!</title><link>https://david.pilato.fr/blog/2018-01-10-5-years-what-a-milestone/</link><pubDate>Wed, 10 Jan 2018 09:00:00 +0100</pubDate><guid>https://david.pilato.fr/blog/2018-01-10-5-years-what-a-milestone/</guid><description>What a milestone! Can you imagine how changed the company in the last 5 years? From 10 employees when I joined to more than 700 now!
If you want to read again my story, it&amp;rsquo;s there:
2013: Once upon a time&amp;hellip; 2014: Once upon a time: a year later&amp;hellip; 2015: Once upon a time: Make your dreams come true 2016: 3 years! Time flies! 2017: 4 years at elastic! Before speaking about what happened the last 5 years for me, let&amp;rsquo;s modify a bit the script I wrote last year.</description></item><item><title>4 years at elastic!</title><link>https://david.pilato.fr/blog/2017-01-10-4-years-at-elastic/</link><pubDate>Mon, 09 Jan 2017 18:15:00 +0100</pubDate><guid>https://david.pilato.fr/blog/2017-01-10-4-years-at-elastic/</guid><description>This post is starting to become a long series ðŸ˜Š
Yeah! That&amp;rsquo;s amazing! I just spent 4 years working at elastic and I&amp;rsquo;m starting my happy 5th year!
If you want to read again my story, it&amp;rsquo;s there:
2013: Once upon a time&amp;hellip; 2014: Once upon a time: a year later&amp;hellip; 2015: Once upon a time: Make your dreams come true 2016: 3 years! Time flies! This year, I will celebrate this by writing a new tutorial&amp;hellip;</description></item><item><title>Elasticsearch real integration tests with security enabled (Updated for GA)</title><link>https://david.pilato.fr/blog/2016-10-21-elasticsearch-real-integration-tests-with-security-enabled-updated-for-ga/</link><pubDate>Fri, 21 Oct 2016 18:24:37 +0200</pubDate><guid>https://david.pilato.fr/blog/2016-10-21-elasticsearch-real-integration-tests-with-security-enabled-updated-for-ga/</guid><description>NOTE: This article is an updated version of Elasticsearch real integration tests with security enabled
In a recent post we have seen how to create real integration tests. Those tests launch a real elasticsearch cluster, then run some tests you write with JUnit or your favorite test framework then stop the cluster.
But sometimes, you may want to add existing plugins in your integration test cluster.
For example, you might want to use X-Pack to bring fantastic features such as:</description></item><item><title>Creating Elasticsearch Transport Action (Updated for GA)</title><link>https://david.pilato.fr/blog/2016-10-20-creating-elasticsearch-transport-action-updated-for-ga/</link><pubDate>Thu, 20 Oct 2016 17:33:29 +0200</pubDate><guid>https://david.pilato.fr/blog/2016-10-20-creating-elasticsearch-transport-action-updated-for-ga/</guid><description>NOTE: This article is an updated version of Creating Elasticsearch Transport Action
This blog post is part of a series which will teach you:
How to write a plugin for elasticsearch 5.0 using Maven. How to add a new REST endpoint plugin to elasticsearch 5.0. How to use Transport Action classes (what you are reading now). How I wrote the ingest-bano plugin which will be hopefully released soonish. In this plugin, new REST endpoints have been added.</description></item><item><title>Adding a new REST endpoint to elasticsearch (Updated for GA)</title><link>https://david.pilato.fr/blog/2016-10-19-adding-a-new-rest-endpoint-to-elasticsearch-updated-for-ga/</link><pubDate>Wed, 19 Oct 2016 14:50:00 +0200</pubDate><guid>https://david.pilato.fr/blog/2016-10-19-adding-a-new-rest-endpoint-to-elasticsearch-updated-for-ga/</guid><description>NOTE: This article is an updated version of Adding a new REST endpoint to elasticsearch
This blog post is part of a series which will teach you:
How to write a plugin for elasticsearch 5.0 using Maven. How to add a new REST endpoint plugin to elasticsearch 5.0 (what you are reading now). How I wrote the ingest-bano plugin which will be hopefully released soonish. In this plugin, new REST endpoints have been added. Imagine that you wish to add a new REST endpoint so you can send requests like:</description></item><item><title>Elasticsearch real integration tests (Updated for GA)</title><link>https://david.pilato.fr/blog/2016-10-18-elasticsearch-real-integration-tests-updated-for-ga/</link><pubDate>Tue, 18 Oct 2016 03:02:48 +0200</pubDate><guid>https://david.pilato.fr/blog/2016-10-18-elasticsearch-real-integration-tests-updated-for-ga/</guid><description>NOTE: This article is an updated version of Elasticsearch real integration tests
Integration tests&amp;hellip; How do you run them?
Often, you are tempted to run services you want to test from JUnit for example. In elasticsearch, you can extend ESIntegTestCase class which will start a cluster of a given number of nodes.
public class BanoPluginIntegrationTest extends ESIntegTestCase { public void testPluginIsLoaded() throws Exception { // Your code here } } But to be honest, the test you are running does not guarantee that you will have the same result in production.</description></item><item><title>Creating an Ingest plugin for elasticsearch (Updated for GA)</title><link>https://david.pilato.fr/blog/2016-10-17-creating-an-ingest-plugin-for-elasticsearch-updated-for-ga/</link><pubDate>Mon, 17 Oct 2016 10:55:29 +0200</pubDate><guid>https://david.pilato.fr/blog/2016-10-17-creating-an-ingest-plugin-for-elasticsearch-updated-for-ga/</guid><description>NOTE: This article is an updated version of Creating an Ingest plugin for elasticsearch
This blog post is part of a series which will teach you:
How to write a plugin for elasticsearch 5.0 using Maven. How to write an ingest plugin for elasticsearch 5.0 (what you are reading now). How I wrote the ingest-bano plugin which will be hopefully released soonish. Today, we will focus on writing an Ingest plugin for elasticsearch.</description></item><item><title>Creating a plugin for elasticsearch 5.0 using Maven (Updated for GA)</title><link>https://david.pilato.fr/blog/2016-10-16-creating-a-plugin-for-elasticsearch-5-dot-0-using-maven-updated-for-ga/</link><pubDate>Sun, 16 Oct 2016 04:00:00 +0200</pubDate><guid>https://david.pilato.fr/blog/2016-10-16-creating-a-plugin-for-elasticsearch-5-dot-0-using-maven-updated-for-ga/</guid><description>NOTE: This article is an updated version of Creating a plugin for elasticsearch 5.0 using Maven
Elasticsearch 5.0 switched to Gradle in October 2015.
You can obviously write a plugin using Gradle if you wish and you could benefit from all the goodies elasticsearch team wrote when it comes to integration tests and so on.
My colleague, Alexander Reelsen aka Spinscale on Twitter, wrote a super nice template if you wish to create an Ingest plugin for 5.</description></item><item><title>Elasticsearch real integration tests with security enabled</title><link>https://david.pilato.fr/blog/2016-08-03-elasticsearch-real-integration-tests-with-security-enabled/</link><pubDate>Wed, 03 Aug 2016 18:24:37 +0200</pubDate><guid>https://david.pilato.fr/blog/2016-08-03-elasticsearch-real-integration-tests-with-security-enabled/</guid><description>NOTE: This article is now outdated. Please read Elasticsearch real integration tests with security enabled (Updated for GA) instead!
In a recent post we have seen how to create real integration tests. Those tests launch a real elasticsearch cluster, then run some tests you write with JUnit or your favorite test framework then stop the cluster.
But sometimes, you may want to add existing plugins in your integration test cluster.
For example, you might want to use X-Pack to bring fantastic features such as:</description></item><item><title>Creating Elasticsearch Transport Action</title><link>https://david.pilato.fr/blog/2016-08-01-creating-elasticsearch-transport-action/</link><pubDate>Mon, 01 Aug 2016 17:33:29 +0200</pubDate><guid>https://david.pilato.fr/blog/2016-08-01-creating-elasticsearch-transport-action/</guid><description>NOTE: This article is now outdated. Please read Creating Elasticsearch Transport Action (Updated for GA) instead!
This blog post is part of a series which will teach you:
How to write a plugin for elasticsearch 5.0 using Maven. How to add a new REST endpoint plugin to elasticsearch 5.0. How to use Transport Action classes (what you are reading now). How I wrote the ingest-bano plugin which will be hopefully released soonish. In this plugin, new REST endpoints have been added.</description></item><item><title>Adding a new REST endpoint to elasticsearch</title><link>https://david.pilato.fr/blog/2016-07-30-adding-a-new-rest-endpoint-to-elasticsearch/</link><pubDate>Sat, 30 Jul 2016 14:50:00 +0200</pubDate><guid>https://david.pilato.fr/blog/2016-07-30-adding-a-new-rest-endpoint-to-elasticsearch/</guid><description>NOTE: This article is now outdated. Please read Adding a new REST endpoint to elasticsearch (Updated for GA) instead!
This blog post is part of a series which will teach you:
How to write a plugin for elasticsearch 5.0 using Maven. How to add a new REST endpoint plugin to elasticsearch 5.0 (what you are reading now). How I wrote the ingest-bano plugin which will be hopefully released soonish. In this plugin, new REST endpoints have been added.</description></item><item><title>Elasticsearch real integration tests</title><link>https://david.pilato.fr/blog/2016-07-29-elasticsearch-real-integration-tests/</link><pubDate>Fri, 29 Jul 2016 03:02:48 +0200</pubDate><guid>https://david.pilato.fr/blog/2016-07-29-elasticsearch-real-integration-tests/</guid><description>NOTE: This article is now outdated. Please read Elasticsearch real integration tests (Updated for GA) instead!
Integration tests&amp;hellip; How do you run them?
Often, you are tempted to run services you want to test from JUnit for example. In elasticsearch, you can extend ESIntegTestCase class which will start a cluster of a given number of nodes.
public class BanoPluginIntegrationTest extends ESIntegTestCase { public void testPluginIsLoaded() throws Exception { // Your code here } } But to be honest, the test you are running does not guarantee that you will have the same result in production.</description></item><item><title>Creating an Ingest plugin for elasticsearch</title><link>https://david.pilato.fr/blog/2016-07-28-creating-an-ingest-plugin-for-elasticsearch/</link><pubDate>Thu, 28 Jul 2016 10:55:29 +0200</pubDate><guid>https://david.pilato.fr/blog/2016-07-28-creating-an-ingest-plugin-for-elasticsearch/</guid><description>NOTE: This article is now outdated. Please read Creating an Ingest plugin for elasticsearch (Updated for GA) instead!
This blog post is part of a series which will teach you:
How to write a plugin for elasticsearch 5.0 using Maven. How to write an ingest plugin for elasticsearch 5.0 (what you are reading now). How I wrote the ingest-bano plugin which will be hopefully released soonish. Today, we will focus on writing an Ingest plugin for elasticsearch.</description></item><item><title>Creating a plugin for elasticsearch 5.0 using Maven</title><link>https://david.pilato.fr/blog/2016-07-27-creating-a-plugin-for-elasticsearch-5-dot-0-using-maven/</link><pubDate>Wed, 27 Jul 2016 16:00:27 +0200</pubDate><guid>https://david.pilato.fr/blog/2016-07-27-creating-a-plugin-for-elasticsearch-5-dot-0-using-maven/</guid><description>NOTE: This article is now outdated. Please read Creating a plugin for elasticsearch 5.0 using Maven (Updated for GA) instead!
Elasticsearch 5.0 switched to Gradle in October 2015.
You can obviously write a plugin using Gradle if you wish and you could benefit from all the goodies elasticsearch team wrote when it comes to integration tests and so on.
My colleague, Alexander Reelsen aka Spinscale on Twitter, wrote a super nice template if you wish to create an Ingest plugin for 5.</description></item><item><title>And the beats go on!</title><link>https://david.pilato.fr/blog/2016-03-17-and-the-beats-go-on/</link><pubDate>Thu, 17 Mar 2016 17:35:39 +0100</pubDate><guid>https://david.pilato.fr/blog/2016-03-17-and-the-beats-go-on/</guid><description>Sounds like a cool music, right? At least this is one of my favorite tracks.
May be some of you already know that, I enjoy doing some DeeJaying for my friends.
But today, I want to speak about another kind of beats. Elastic beats!
Elastic Beats
Actually my favorite funky music track is a one from Georges Duke: Reach out! But this is another story&amp;hellip;
Beats So what are beats?</description></item><item><title>Understanding Zipf's law</title><link>https://david.pilato.fr/blog/2016-01-05-understanding-zipfs-law/</link><pubDate>Tue, 05 Jan 2016 12:13:02 +0100</pubDate><guid>https://david.pilato.fr/blog/2016-01-05-understanding-zipfs-law/</guid><description>I just discovered a nice video which explains the Zipf&amp;rsquo;s law.
I&amp;rsquo;m wondering if I can index the french lexique from UniversitÃ© de Savoie and find some funny things based on that&amp;hellip;
Download french words wget http://www.lexique.org/listes/liste_mots.txt head -20 liste_mots.txt What do we have?
It&amp;rsquo;s a CSV file (tabulation as separator):
1_graph 8_frantfreqparm 0 279.84 1 612.10 2 1043.90 3 839.32 4 832.23 5 913.87 6 603.42 7 600.61 8 908.03 9 1427.</description></item><item><title>Building a directory map with ELK</title><link>https://david.pilato.fr/blog/2015-12-10-building-a-directory-map-with-elk/</link><pubDate>Thu, 10 Dec 2015 09:28:15 +0100</pubDate><guid>https://david.pilato.fr/blog/2015-12-10-building-a-directory-map-with-elk/</guid><description>I gave a BBL talk recently and while chatting with attendees, one of them told me a simple use case he covered with elasticsearch: indexing metadata files on a NAS with a simple ls -lR like command. His need is to be able to search on a NAS for files when a user wants to restore a deleted file.
As you can imagine a search engine is super helpful when you have hundreds of millions files!</description></item><item><title>Index Twitter on found</title><link>https://david.pilato.fr/blog/2015-11-17-index-twitter-on-found/</link><pubDate>Tue, 17 Nov 2015 11:51:43 +0100</pubDate><guid>https://david.pilato.fr/blog/2015-11-17-index-twitter-on-found/</guid><description>Some months ago, I published a recipe on how to index Twitter with Logstash and Elasticsearch.
I have the same need today as I want to monitor Twitter when we run the elastic FR meetup (join us by the way if you are in France!).
Well, this recipe can be really simplified and actually I don&amp;rsquo;t want to waste my time anymore on building and managing elasticsearch and Kibana clusters anymore.
Let&amp;rsquo;s use a Found by elastic cluster instead.</description></item><item><title>Next movie to watch based on recommendation</title><link>https://david.pilato.fr/blog/2015-09-17-next-movie-to-watch-based-on-recommendation/</link><pubDate>Thu, 17 Sep 2015 14:42:05 +0200</pubDate><guid>https://david.pilato.fr/blog/2015-09-17-next-movie-to-watch-based-on-recommendation/</guid><description>This article is based on Recommender System with Mahout and Elasticsearch tutorial created by MapR.
It now uses the 20M MovieLens dataset which contains: 20 million ratings and 465 000 tag applications applied to 27 000 movies by 138 000 users and was released in 4/2015. The format with this recent version has changed a bit so I needed to adapt the existing scripts to the new format.
Prerequisites Download the 20M MovieLens dataset. Unzip it.</description></item><item><title>Importing from a database without a database</title><link>https://david.pilato.fr/blog/2015-09-14-import-from-sql-without-database/</link><pubDate>Mon, 14 Sep 2015 11:05:00 +0200</pubDate><guid>https://david.pilato.fr/blog/2015-09-14-import-from-sql-without-database/</guid><description>Recently, I got a database MySQL dump and I was thinking of importing it into elasticsearch.
The first idea which pops up was:
install MySQL import the database read the database with Logstash and import into elasticsearch drop the database uninstall MySQL Well. I found that some of the steps are really not needed.
I can actually use ELK stack and create a simple recipe which can be used to import SQL dump scripts without needing to actually load the data to a database and then read it again from the database.</description></item><item><title>Indexing Twitter with Logstash and Elasticsearch</title><link>https://david.pilato.fr/blog/2015-06-01-indexing-twitter-with-logstash-and-elasticsearch/</link><pubDate>Mon, 01 Jun 2015 16:23:03 +0200</pubDate><guid>https://david.pilato.fr/blog/2015-06-01-indexing-twitter-with-logstash-and-elasticsearch/</guid><description>I&amp;rsquo;m often running some demos during conferences where we have a booth. As many others, I&amp;rsquo;m using Twitter feed as my datasource.
I have been using Twitter river plugin for many years but, you know, rivers have been deprecated.
Logstash 1.5.0 provides a safer and more flexible way to deal with tweets with its twitter input.
Let&amp;rsquo;s do it!
Let&amp;rsquo;s assume that you have already elasticsearch 1.5.2, Logstash 1.5.0 and Kibana 4.0.2 running on your laptop or on a cloud instance.</description></item><item><title>Reindex elasticsearch with Logstash</title><link>https://david.pilato.fr/blog/2015-05-20-reindex-elasticsearch-with-logstash/</link><pubDate>Wed, 20 May 2015 11:03:29 +0200</pubDate><guid>https://david.pilato.fr/blog/2015-05-20-reindex-elasticsearch-with-logstash/</guid><description>Sometimes, you would like to reindex your data to change your mapping or to change your index settings or to move from one server to another or to one cluster to another (think about multiple data centers for example).
For the later you can use Snapshot and Restore feature but if you need to change any index settings, you need something else.
With Logstash 1.5.0, you can now do it super easily using elasticsearch input and elasticsearch output.</description></item><item><title>Using JS Auth with found cluster</title><link>https://david.pilato.fr/blog/2015-05-19-using-js-auth-with-found-cluster/</link><pubDate>Tue, 19 May 2015 17:06:22 +0200</pubDate><guid>https://david.pilato.fr/blog/2015-05-19-using-js-auth-with-found-cluster/</guid><description>Using Found by elastic cluster helps a lot to have a ready to use and managed elasticsearch cluster.
I started my own cluster yesterday to power brownbaglunch.fr website (work in progress) and it was ready to use after some clicks!
It&amp;rsquo;s a kind of magic!
But I ran into an issue when you secure it and use the elasticsearch javascript client.
Creating your cluster Found Console
Adding ACL By default, your cluster is opened but you can fix that by opening &amp;ldquo;Access Control&amp;rdquo; menu.</description></item><item><title>Advanced search for your Legacy application</title><link>https://david.pilato.fr/blog/2015-05-09-advanced-search-for-your-legacy-application/</link><pubDate>Sat, 09 May 2015 14:15:05 +0300</pubDate><guid>https://david.pilato.fr/blog/2015-05-09-advanced-search-for-your-legacy-application/</guid><description>I gave recently a talk at Voxxed Istanbul 2015 and I&amp;rsquo;d like to share here the story of this talk.
The talk was about adding a real search engine for your legacy application. Here &amp;ldquo;legacy&amp;rdquo; means an application which is still using SQL statements to execute search requests.
Our current CRM application can visualize our customers. Each person is represented as a Person bean and have some properties like name, dateOfBirth, children, country, city and some metrics related to the number of clicks each person did on the car or food buttons on our mobile application (center of interests that is).</description></item><item><title>Devoxx France 2015</title><link>https://david.pilato.fr/blog/2015-05-02-devoxx-france-2015/</link><pubDate>Sat, 02 May 2015 01:30:00 +0200</pubDate><guid>https://david.pilato.fr/blog/2015-05-02-devoxx-france-2015/</guid><description>I gave recently a talk at Devoxx France 2015 with Colin Surprenant and I&amp;rsquo;d like to share here some of the examples we used for the talk.
The talk was about &amp;ldquo;what my data look like?&amp;rdquo;.
We said that our manager was asking us to answer some questions:
who are our customers? how do they use our services? what do they think about us on Twitter? Our CRM database So we have a PostgreSQL database containing our data.</description></item><item><title>Exploring Capitaine Train dataset</title><link>https://david.pilato.fr/blog/2015-04-28-exploring-capitaine-train-dataset/</link><pubDate>Tue, 28 Apr 2015 13:31:34 +0200</pubDate><guid>https://david.pilato.fr/blog/2015-04-28-exploring-capitaine-train-dataset/</guid><description>Recently I saw a tweet where Capitaine Train team started to open data they have collected and enriched or corrected.
Ouvrez, ouvrez, les donnÃ©es structurÃ©es. Capitaine Train libÃ¨re les gares : https://t.co/y6DjWsbALF #opendata
&amp;mdash; Trainline France (@trainline_fr) April 23, 2015 I decided to play a bit with ELK stack and create a simple recipe which can be used with any other CSV like data.
Prerequisites You will need:
Logstash: I&amp;rsquo;m using 1.5.0-rc3. Elasticsearch: I&amp;rsquo;m using 1.</description></item><item><title>ProtÃ©ger son cluster Elasticsearch avec Jetty</title><link>https://david.pilato.fr/blog/2012-04-10-proteger-son-cluster-elasticsearch-avec-jetty/</link><pubDate>Tue, 10 Apr 2012 20:29:29 +0000</pubDate><guid>https://david.pilato.fr/blog/2012-04-10-proteger-son-cluster-elasticsearch-avec-jetty/</guid><description>Nativement, Elasticsearch expose l&amp;rsquo;ensemble de ses services sans aucune authentification et donc une commande du type curl -XDELETE http://localhost:9200/myindex peut faire de nombreux dÃ©gÃ¢ts non dÃ©sirÃ©s.
De plus, si vous dÃ©veloppez une application JQuery avec un accÃ¨s direct depuis le poste client Ã  votre cluster Elasticsearch, le risque qu&amp;rsquo;un utilisateur joue un peu avec votre cluster est grand !
Alors, pas de panique&amp;hellip; La sociÃ©tÃ© Sonian Inc. a open sourcÃ© son plugin Jetty pour Elasticsearch pour notre plus grand bonheur ðŸ˜‰</description></item><item><title>Quel client Java pour elasticsearch ?</title><link>https://david.pilato.fr/blog/2012-02-13-quel-client-java-pour-elasticsearch/</link><pubDate>Mon, 13 Feb 2012 21:37:12 +0000</pubDate><guid>https://david.pilato.fr/blog/2012-02-13-quel-client-java-pour-elasticsearch/</guid><description>Il existe deux modes d&amp;rsquo;accÃ¨s Ã  elasticsearch en Java :
Inscrire un noeud client dans le cluster elasticsearch Utiliser un client &amp;ldquo;simple&amp;rdquo; Noeud client dans un cluster elasticsearch L&amp;rsquo;idÃ©e de cette mÃ©thode est de fabriquer un noeud elasticsearch (node) qui dÃ©marre avec les mÃªmes caractÃ©ristiques qu&amp;rsquo;un noeud d&amp;rsquo;indexation et de recherche sauf qu&amp;rsquo;on lui prÃ©cise qu&amp;rsquo;il n&amp;rsquo;hÃ©bergera pas de donnÃ©es.
Pour cela, on utilise la propriÃ©tÃ© suivante :
node.data=false Elle indique que le noeud que nous dÃ©marrons n&amp;rsquo;hÃ©bergera pas de donnÃ©es.</description></item><item><title>Elasticsearch et les "facets"</title><link>https://david.pilato.fr/blog/2011-06-20-elasticsearch-et-les-facets/</link><pubDate>Mon, 20 Jun 2011 20:34:49 +0000</pubDate><guid>https://david.pilato.fr/blog/2011-06-20-elasticsearch-et-les-facets/</guid><description>Les aventures avec Elasticsearch se poursuivent.
Combien de fois ai-je dit rÃ©cemment que ce projet est absolument gÃ©nial et qu&amp;rsquo;il va constituer Ã  mon sens un des projets majeurs des prochaines annÃ©es&amp;hellip;
Qui n&amp;rsquo;a pas besoin de moteur de recherche ? Qui s&amp;rsquo;est dÃ©jÃ  &amp;ldquo;emmerdÃ©&amp;rdquo; Ã  fabriquer Ã§a lui-mÃªme ou Ã  utiliser des briques pouvant aider au prix d&amp;rsquo;une complexitÃ© plus ou moins grande de mise en oeuvre ?
Je crois que nous sommes tous passÃ©s par lÃ  !</description></item><item><title>CouchDB</title><link>https://david.pilato.fr/blog/2011-05-13-couchdb/</link><pubDate>Fri, 13 May 2011 21:08:22 +0000</pubDate><guid>https://david.pilato.fr/blog/2011-05-13-couchdb/</guid><description>AprÃ¨s avoir testÃ© Elasticsearch, me voici parti pour regarder ce monde Ã©trange qu&amp;rsquo;on appelle le NoSQL&amp;hellip;
A dire vrai, j&amp;rsquo;ai entendu ce mot il y a quelques annÃ©es, sans jamais vraiment m&amp;rsquo;y interesser&amp;hellip; AprÃ¨s tout, une base de donnÃ©es non SQL, Ã§a n&amp;rsquo;est tout simplement pas possible !!!
Puis, Ã  force de cotoyer le monde d&amp;rsquo;Elasticsearch et les technos JSon et REST, je me lance.
Pour des raisons trÃ¨s pratiques, je choisis CouchDB de Apache. D&amp;rsquo;une part, il est directement intÃ©grable avec Elasticsearch, et Ã  la lecture rapide de sa documentation, il semble rÃ©pondre Ã  un des besoins auquel une Ã©quipe de mon pÃ´le de dÃ©veloppement est confrontÃ©e.</description></item><item><title>La recherche Ã©lastique...</title><link>https://david.pilato.fr/blog/2011-03-09-la-recherche-elastique/</link><pubDate>Wed, 09 Mar 2011 21:30:32 +0000</pubDate><guid>https://david.pilato.fr/blog/2011-03-09-la-recherche-elastique/</guid><description>Elasticsearch, un projet mature en quelques mois&amp;hellip; A suivre de trÃ¨s prÃ¨s !
En cherchant un bout de code pour rendre la couche Hibernate Search facilement distribuable sur un cluster de machines JBoss, je suis tombÃ© sur le projet Elasticsearch.
Au dÃ©but, un peu interloquÃ©&amp;hellip; Puis, je me lance&amp;hellip;
Je tÃ©lÃ©charge le projet. Je dÃ©zippe.
Je lance&amp;hellip;
Miracle. En quelques secondes, je dispose d&amp;rsquo;un outil dans un Cloud, simple, me permettant d&amp;rsquo;indexer n&amp;rsquo;importe quel type de document, de le rÃ©cupÃ©rer et de faire une recherche (au sens google du terme) sur n&amp;rsquo;importe quel champ&amp;hellip; Et cela, quelque soit la technologie employÃ©e (Java, C#, .</description></item><item><title>Installation FusionForge 5.0 sur Redhat 5</title><link>https://david.pilato.fr/blog/2010-05-11-installation-fusionforge-5-0-sur-redhat-5/</link><pubDate>Tue, 11 May 2010 22:23:27 +0000</pubDate><guid>https://david.pilato.fr/blog/2010-05-11-installation-fusionforge-5-0-sur-redhat-5/</guid><description>Voici la suite de l&amp;rsquo;article sur l'installation d&amp;rsquo;une forge.
Finalement, le temps d&amp;rsquo;obtenir une machine sous Redhat 5 a laissÃ© le temps Ã  la team FusionForge de sortir une release finale de la version 5.0.
Nous voilÃ  donc lancÃ©s dans cette installation que je me propose de dÃ©crire ici.
A noter que pour le moment la forge n&amp;rsquo;est pas totalement opÃ©rationnelle. Des Ã©volutions dans la configuration devront Ãªtre menÃ©es et j&amp;rsquo;espÃ¨re pouvoir tenir Ã  jour cet article pour les dÃ©crire.</description></item><item><title>Utilisation du mode Lazy d'Hibernate avec Struts et Spring</title><link>https://david.pilato.fr/blog/2010-02-24-utilisation-du-mode-lazy-dhibernate-avec-struts-et-spring/</link><pubDate>Wed, 24 Feb 2010 13:56:56 +0000</pubDate><guid>https://david.pilato.fr/blog/2010-02-24-utilisation-du-mode-lazy-dhibernate-avec-struts-et-spring/</guid><description>Lorsqu&amp;rsquo;on utilise Hibernate pour dÃ©lÃ©guer la gestion de la persistence, se pose alors le classique problÃ¨me de l&amp;rsquo;exception LazyInitialisationException.
En effet, dans une modÃ©lisation assez classique, imaginons le cas suivant :
Couche ModÃ¨le (ou DAO) Classe POJO contenant un attribut x et une collection cols @Entity @Inheritance(strategy=InheritanceType.SINGLE_TABLE) public class Dossier { @Id @GeneratedValue private Long id; private String x; @OneToMany(cascade=CascadeType.ALL) private Collections cols; // Getter et setters } Classe DAO Voir le blog pour l&amp;rsquo;utilisation des generics de Java5 afin d&amp;rsquo;Ã©viter d&amp;rsquo;avoir Ã  coder toujours les mÃªmes mÃ©thodes CRUD.</description></item><item><title>Publication de documentation fonctionnelle avec Maven</title><link>https://david.pilato.fr/blog/2010-02-23-publication-de-documentation-fonctionnelle-avec-maven/</link><pubDate>Tue, 23 Feb 2010 12:40:26 +0000</pubDate><guid>https://david.pilato.fr/blog/2010-02-23-publication-de-documentation-fonctionnelle-avec-maven/</guid><description>&lt;p>Voici une astuce permettant de laisser les analystes ou concepteurs utiliser leurs logiciels habituels de documentation (oOo ou Word), tout en permettant de publier automatiquement avec la gÃ©nÃ©ration du site un document PDF lisible par tous.&lt;/p></description></item><item><title>La mise en place d'une forge</title><link>https://david.pilato.fr/blog/2010-01-26-la-mise-en-place-dune-forge/</link><pubDate>Tue, 26 Jan 2010 22:13:47 +0000</pubDate><guid>https://david.pilato.fr/blog/2010-01-26-la-mise-en-place-dune-forge/</guid><description>Description de la mise en place de la forge GForge pour les besoins de mon centre informatique.
Pour les besoins internes de la douane, j&amp;rsquo;ai proposÃ© la mise en place d&amp;rsquo;une forge afin de consolider nos moyens de dÃ©veloppement et de gestion de projets.
Histoire d&amp;rsquo;Ãªtre cohÃ©rent avec d&amp;rsquo;autres choix faits par l&amp;rsquo;administration, projet Adullact, j&amp;rsquo;ai retenu la forge GFORGE.
Je vais dÃ©crire ici le processus d&amp;rsquo;installation que je vais suivre afin de partager cette information avec d&amp;rsquo;autres personnes qui pourraient Ãªtre intÃ©ressÃ©s par cette dÃ©marche.</description></item><item><title>DÃ©couverte de Google App Engine pour Java</title><link>https://david.pilato.fr/blog/2010-01-23-decouverte-de-google-app-engine-pour-java/</link><pubDate>Sat, 23 Jan 2010 13:34:32 +0000</pubDate><guid>https://david.pilato.fr/blog/2010-01-23-decouverte-de-google-app-engine-pour-java/</guid><description>&lt;p>Je viens de dÃ©couvrir Google App Engine pour Java. Je vais essayer de complÃ©ter cet article au fur et Ã  mesure que je vais avancer dans son utilisation&amp;hellip;&lt;/p>
&lt;p>Stay tuned&amp;hellip;&lt;/p></description></item></channel></rss>