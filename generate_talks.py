#!/usr/bin/env python3
"""
Script pour g√©n√©rer les talks √† partir de l'API Notist de speaker.pilato.fr
"""

import json
import os
import re
import urllib.request
import urllib.error
import ssl
import html
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path

# Configuration
BASE_URL = "https://speaker.pilato.fr"
TALKS_DIR = Path(__file__).parent / "content" / "talks"
EXISTING_TALKS = set()

# Cr√©er un contexte SSL qui ne v√©rifie pas les certificats
ssl_context = ssl.create_default_context()
ssl_context.check_hostname = False
ssl_context.verify_mode = ssl.CERT_NONE


def slugify(text):
    """Convertit un texte en slug URL-friendly."""
    text = text.lower()
    text = re.sub(r'[√†√°√¢√£√§√•]', 'a', text)
    text = re.sub(r'[√®√©√™√´]', 'e', text)
    text = re.sub(r'[√¨√≠√Æ√Ø]', 'i', text)
    text = re.sub(r'[√≤√≥√¥√µ√∂]', 'o', text)
    text = re.sub(r'[√π√∫√ª√º]', 'u', text)
    text = re.sub(r'[√Ω√ø]', 'y', text)
    text = re.sub(r'[√±]', 'n', text)
    text = re.sub(r'[√ß]', 'c', text)
    text = re.sub(r"[''`]", '-', text)
    text = re.sub(r'[^\w\s-]', '', text)
    text = re.sub(r'[-\s]+', '-', text)
    return text.strip('-')


def fetch_json(url):
    """R√©cup√®re le JSON depuis une URL."""
    try:
        req = urllib.request.Request(url, headers={'User-Agent': 'Mozilla/5.0'})
        with urllib.request.urlopen(req, context=ssl_context, timeout=30) as response:
            return json.loads(response.read().decode('utf-8'))
    except Exception as e:
        print(f"  ‚ö†Ô∏è  Erreur lors de la r√©cup√©ration de {url}: {e}")
        return None


def download_file(url, dest_path):
    """T√©l√©charge un fichier."""
    try:
        req = urllib.request.Request(url, headers={'User-Agent': 'Mozilla/5.0'})
        with urllib.request.urlopen(req, context=ssl_context, timeout=60) as response:
            with open(dest_path, 'wb') as f:
                f.write(response.read())
        return True
    except Exception as e:
        print(f"  ‚ö†Ô∏è  Erreur t√©l√©chargement {url}: {e}")
        return False


def html_to_text(html_content):
    """Convertit le HTML en texte simple."""
    if not html_content:
        return ""
    # Supprimer les balises HTML
    text = re.sub(r'<[^>]+>', '', html_content)
    # D√©coder les entit√©s HTML
    text = html.unescape(text)
    return text.strip()


def extract_city_from_address(address):
    """Extrait la ville de l'adresse."""
    if not address:
        return ""
    # Format typique: "21000 Dijon, France" ou "Paris, France"
    parts = address.split(',')
    if len(parts) >= 1:
        city_part = parts[0].strip()
        # Enlever le code postal s'il y en a
        match = re.search(r'(?:\d{5}\s+)?(.+)', city_part)
        if match:
            return match.group(1).strip()
    return address


def get_country_name(country_code):
    """Retourne le nom du pays √† partir du code."""
    countries = {
        'FR': 'France',
        'BE': 'Belgium',
        'CH': 'Switzerland',
        'CA': 'Canada',
        'GB': 'United Kingdom',
        'UK': 'United Kingdom',
        'US': 'United States',
        'DE': 'Germany',
        'ES': 'Spain',
        'IT': 'Italy',
        'NL': 'Netherlands',
        'PL': 'Poland',
        'PT': 'Portugal',
        'LU': 'Luxembourg',
        'SG': 'Singapore',
        'MU': 'Mauritius',
        'BG': 'Bulgaria',
        'RS': 'Serbia',
        'MA': 'Morocco',
    }
    return countries.get(country_code.upper(), country_code)


def process_presentation(pres_data):
    """Traite une pr√©sentation et g√©n√®re le talk."""
    pres_id = pres_data['id'].replace('pr_', '')
    title = pres_data['attributes']['title']
    presented_on = pres_data['attributes']['presented_on']
    
    # Extraire la date
    date = datetime.strptime(presented_on.split(' ')[0], '%Y-%m-%d')
    date_str = date.strftime('%Y-%m-%d')
    
    # R√©cup√©rer les d√©tails complets de la pr√©sentation
    detail_url = f"{BASE_URL}/{pres_id}.json"
    details = fetch_json(detail_url)
    
    if not details or 'data' not in details or not details['data']:
        print(f"  ‚ö†Ô∏è  Impossible de r√©cup√©rer les d√©tails pour {pres_id}")
        return None
    
    detail = details['data'][0]
    attrs = detail['attributes']
    
    # Extraire les infos de l'√©v√©nement
    event_info = {}
    if 'relationships' in detail and 'data' in detail['relationships']:
        events = detail['relationships']['data']
        if events:
            event = events[0]['attributes']
            event_info = {
                'name': event.get('title', ''),
                'url': event.get('url', ''),
                'city': extract_city_from_address(event.get('address', '')),
                'country': get_country_name(event.get('country_code', 'FR')),
                'country_code': event.get('country_code', 'fr').lower(),
            }
    
    # Cr√©er le slug pour le r√©pertoire
    event_slug = slugify(event_info.get('name', 'unknown'))
    dir_name = f"{date_str}-{event_slug}"
    talk_dir = TALKS_DIR / dir_name
    
    # V√©rifier si le talk existe d√©j√†
    if talk_dir.exists():
        print(f"  ‚è≠Ô∏è  Talk d√©j√† existant: {dir_name}")
        return dir_name
    
    # Cr√©er le r√©pertoire
    talk_dir.mkdir(parents=True, exist_ok=True)
    
    # R√©cup√©rer l'URL de la premi√®re slide pour la couverture
    cover_url = None
    if 'slidedeck' in attrs and 'data' in attrs['slidedeck'] and attrs['slidedeck']['data']:
        slides_data = attrs['slidedeck']['data'][0]
        if 'slides' in slides_data and slides_data['slides']:
            cover_url = slides_data['slides'][0].get('image')
    
    # R√©cup√©rer l'URL du PDF
    pdf_url = attrs.get('download')
    
    # T√©l√©charger la couverture
    if cover_url:
        cover_ext = 'png' if cover_url.endswith('.png') else 'jpg'
        cover_path = talk_dir / f"cover.{cover_ext}"
        if download_file(cover_url, cover_path):
            print(f"  üì∏ Image t√©l√©charg√©e: {cover_path.name}")
    
    # T√©l√©charger le PDF
    pdf_path = None
    if pdf_url:
        pdf_name = f"{dir_name}.pdf"
        pdf_path = talk_dir / pdf_name
        if download_file(pdf_url, pdf_path):
            print(f"  üìÑ PDF t√©l√©charg√©: {pdf_name}")
    
    # Extraire la description
    description = ""
    if 'blurb' in attrs and attrs['blurb']:
        description = html_to_text(attrs['blurb'].get('html', ''))
    
    # G√©n√©rer le fichier index.md
    cover_file = None
    for ext in ['png', 'jpg', 'jpeg', 'JPG', 'PNG']:
        if (talk_dir / f"cover.{ext}").exists():
            cover_file = f"cover.{ext}"
            break
    
    # Corriger l'URL si elle est None
    event_url = event_info.get('url', '') or ''
    if event_url == 'None':
        event_url = ''
    
    frontmatter = f'''---
title: "{title}"
description: ""
conference: 
  name: "{event_info.get('name', '')}"
  url: "{event_url}"
  city: "{event_info.get('city', '')}"
  country: "{event_info.get('country', '')}"
  country_code: "{event_info.get('country_code', 'fr')}"
author: David Pilato
avatar: /about/david_pilato.png
tags:
  - elasticsearch
  - conference
  - java
  - cloud
categories:
  - speaker
series:
  - conferences
date: {date_str}
nolastmod: true
draft: false
'''
    
    if cover_file:
        frontmatter += f'cover: {cover_file}\n'
    
    frontmatter += f'''
# Speaker specific fields
#youtube: ""
notist: "dadoonet/{pres_id}"
---

{description}
'''
    
    index_path = talk_dir / "index.md"
    with open(index_path, 'w', encoding='utf-8') as f:
        f.write(frontmatter)
    
    print(f"‚úÖ Talk cr√©√©: {dir_name}")
    return dir_name


def main():
    print("üöÄ G√©n√©ration des talks depuis speaker.pilato.fr\n")
    
    # R√©cup√©rer la liste des pr√©sentations
    print("üì• R√©cup√©ration de la liste des pr√©sentations...")
    presentations_url = f"{BASE_URL}/presentations.json"
    data = fetch_json(presentations_url)
    
    if not data or 'data' not in data:
        print("‚ùå Impossible de r√©cup√©rer les pr√©sentations")
        return
    
    presentations = data['data']
    print(f"üìä {len(presentations)} pr√©sentations trouv√©es\n")
    
    # Lister les talks existants
    if TALKS_DIR.exists():
        for item in TALKS_DIR.iterdir():
            if item.is_dir() and not item.name.startswith('_'):
                EXISTING_TALKS.add(item.name)
    print(f"üìÅ {len(EXISTING_TALKS)} talks existants\n")
    
    # Traiter chaque pr√©sentation
    created = 0
    skipped = 0
    errors = 0
    
    for i, pres in enumerate(presentations, 1):
        title = pres['attributes']['title']
        print(f"\n[{i}/{len(presentations)}] {title}")
        
        try:
            result = process_presentation(pres)
            if result:
                if result in EXISTING_TALKS:
                    skipped += 1
                else:
                    created += 1
                    EXISTING_TALKS.add(result)
            else:
                errors += 1
        except Exception as e:
            print(f"  ‚ùå Erreur: {e}")
            errors += 1
    
    print(f"\n{'='*50}")
    print(f"üìä R√©sum√©:")
    print(f"   - Cr√©√©s: {created}")
    print(f"   - Existants (ignor√©s): {skipped}")
    print(f"   - Erreurs: {errors}")
    print(f"{'='*50}")


if __name__ == "__main__":
    main()

